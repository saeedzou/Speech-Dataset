{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "NEMO_DIR_PATH = \"./NeMo\"\n",
    "TOOLS_DIR = f'{NEMO_DIR_PATH}/tools/ctc_segmentation/scripts'\n",
    "WORK_DIR = 'WORK_DIR'\n",
    "DATA_DIR = WORK_DIR + '/DATA'\n",
    "OUTPUT_DIR = WORK_DIR + \"/output\"\n",
    "\n",
    "import os\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/audio', exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/text', exist_ok=True)\n",
    "\n",
    "! git clone https://github.com/saeedzou/NeMo.git\n",
    "!cd NeMo && bash colab_install.sh && apt-get install -y sox libsox-fmt-mp3 ffmpeg\n",
    "\n",
    "! pip install -q wtpsplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data in the following format:\n",
    "- Audios must be under {DATA_DIR}/audio\n",
    "- Texts must be under {DATA_DIR}/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gdown\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import subprocess\n",
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wtpsplit import SaT\n",
    "from IPython.display import Audio, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def initialize_sat(model_name=\"sat-12l-sm\"):\n",
    "    sat = SaT(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        sat.half().to(\"cuda\")\n",
    "    return sat\n",
    "\n",
    "NVIDIA_FASTCONFORMER = \"nvidia/stt_fa_fastconformer_hybrid_large\"\n",
    "WAV2VEC2_FA = \"masoudmzb/wav2vec2-xlsr-multilingual-53-fa\"\n",
    "WAV2VEC2_V3 = \"m3hrdadfi/wav2vec2-large-xlsr-persian-v3\"\n",
    "WHISPER_TINY = \"openai/whisper-tiny\"\n",
    "WHISPER_BASE = \"openai/whisper-base\"\n",
    "WHISPER_SMALL = \"openai/whisper-small\"\n",
    "WHISPER_MEDIUM = \"openai/whisper-medium\"\n",
    "WHISPER_LARGE = \"openai/whisper-large\"\n",
    "HEZAR = 'hezarai/whisper-small-fa'\n",
    "MODELS_DICT = {\n",
    "    \"nvidia_stt_fa_fastconformer_hybrid_large\": \"NVIDIA_FASTCONFORMER\",\n",
    "    \"masoudmzb_wav2vec2_xlsr_multilingual_53_fa\": \"WAV2VEC2_FA\",\n",
    "    \"m3hrdadfi_wav2vec2_large_xlsr_persian_v3\": \"WAV2VEC2_V3\",\n",
    "    \"openai_whisper_tiny\": \"WHISPER_TINY\",\n",
    "    \"openai_whisper_base\": \"WHISPER_BASE\",\n",
    "    \"openai_whisper_small\": \"WHISPER_SMALL\",\n",
    "    \"openai_whisper_medium\": \"WHISPER_MEDIUM\",\n",
    "    \"openai_whisper_large\": \"WHISPER_LARGE\",\n",
    "    \"hezarai_whisper_small_fa\": \"HEZAR\",\n",
    "}\n",
    "MODELS = f\"{NVIDIA_FASTCONFORMER} {WAV2VEC2_V3} {HEZAR}\"\n",
    "\n",
    "MODE = \"\"\n",
    "LANG_ID='fa'\n",
    "OFFSET = 0\n",
    "THRESHOLD = -2\n",
    "WINDOW = 8000\n",
    "CER_THRESHOLD = 40\n",
    "WER_THRESHOLD = 75\n",
    "CER_EDGE_THRESHOLD = 75\n",
    "LEN_DIFF_RATIO_THRESHOLD = 0.4\n",
    "MIN_DURATION = 1\n",
    "MAX_DURATION = 20\n",
    "EDGE_LEN = 7\n",
    "OUTPUT_FORMAT = 'wav'\n",
    "REMOVE_BRACKETS=True\n",
    "REMOVE_ASTERISKS=True\n",
    "REMOVE_PARENTHESES=True\n",
    "REMOVE_SPEAKER_LABELS=False\n",
    "SPLIT_USING_PATTERN=False\n",
    "SPLIT_ON_QUOTES=False\n",
    "SPLIT_ON_VERBS=True\n",
    "SPLIT_ON_VERBS_MIN_WORDS=10\n",
    "SPLIT_ON_VERBS_MAX_WORDS=24\n",
    "ADDITIONAL_SPLIT_SYMBOLS=\":\\|\\?\\|\\!\\|\\؟\\|\\?”\\|\\!”\" # add new symbols, separated by | (\\| before ? and ! and . and | because they are special characters in regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script does the following:\n",
    "1. Prepares data in the right format for CTC segmentation i.e. newline separated text (roughly an utterance) and 16000 Hz mono audio in `.wav` format for the NeMo ASR model\n",
    "\n",
    "2. Runs CTC segmentation on the processed data and outputs segments text file for each audio file containing utterance start, end timings and alignment score\n",
    "\n",
    "3. Verifies the segments created in step 2\n",
    "\n",
    "4. Cuts the audios into utterances and creates a json manifest file (NeMo format) of the information of each utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "text_names = []\n",
    "for file in os.listdir(f\"{DATA_DIR}/text\"):\n",
    "    if file.endswith('.txt'):\n",
    "        text_names.append(os.path.join(f\"{DATA_DIR}/text\", file))\n",
    "        texts.append(open(os.path.join(f\"{DATA_DIR}/text\", file), 'r', encoding='utf-8').read().replace(\"\\n\", \" \"))\n",
    "\n",
    "sat = initialize_sat()\n",
    "texts = sat.split(texts)\n",
    "for text_name, text in zip(text_names, texts):\n",
    "    text = \"\\n\".join(text)\n",
    "    with open(os.path.join(f\"{DATA_DIR}/text\", text_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "del sat\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $OUTPUT_DIR\n",
    "\n",
    "! bash $TOOLS_DIR/../run_segmentation.sh \\\n",
    "--MODEL_NAME_OR_PATH=$NVIDIA_FASTCONFORMER \\\n",
    "--DATA_DIR=$DATA_DIR \\\n",
    "--OUTPUT_DIR=$OUTPUT_DIR \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--REMOVE_BRACKETS=$REMOVE_BRACKETS \\\n",
    "--REMOVE_ASTERISKS=$REMOVE_ASTERISKS \\\n",
    "--REMOVE_PARANTHESES=$REMOVE_PARENTHESES \\\n",
    "--REMOVE_SPEAKER_LABELS=$REMOVE_SPEAKER_LABELS \\\n",
    "--SPLIT_USING_PATTERN=$SPLIT_USING_PATTERN \\\n",
    "--SPLIT_ON_QUOTES=$SPLIT_ON_QUOTES \\\n",
    "--SPLIT_ON_VERBS=$SPLIT_ON_VERBS \\\n",
    "--SPLIT_ON_VERBS_MIN_WORDS=$SPLIT_ON_VERBS_MIN_WORDS \\\n",
    "--SPLIT_ON_VERBS_MAX_WORDS=$SPLIT_ON_VERBS_MAX_WORDS \\\n",
    "--ADDITIONAL_SPLIT_SYMBOLS=$ADDITIONAL_SPLIT_SYMBOLS \\\n",
    "--LANGUAGE=$LANG_ID \\\n",
    "--MIN_SCORE=$THRESHOLD  \\\n",
    "--USE_NEMO_NORMALIZATION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Transcribes the segments created from the `run_segmentation.sh` using the generated manifest for models in `MODELS` and calculate metrics such as WER, CER, etc. and outputs a new manifest file for each model\n",
    "\n",
    "2. Filters out segments that don't meet the minimum requirements of any of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash $TOOLS_DIR/../run_filter_multiple.sh \\\n",
    "--MODEL_NAME_OR_PATH=\"$MODELS\" \\\n",
    "--INPUT_AUDIO_DIR=$DATA_DIR/audio \\\n",
    "--MANIFEST=$OUTPUT_DIR/manifests/manifest.json \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--CER_THRESHOLD=$CER_THRESHOLD \\\n",
    "--WER_THRESHOLD=$WER_THRESHOLD \\\n",
    "--CER_EDGE_THRESHOLD=$CER_EDGE_THRESHOLD \\\n",
    "--LEN_DIFF_RATIO_THRESHOLD=$LEN_DIFF_RATIO_THRESHOLD \\\n",
    "--MIN_DURATION=$MIN_DURATION \\\n",
    "--MAX_DURATION=$MAX_DURATION \\\n",
    "--EDGE_LEN=$EDGE_LEN || exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the JSON file\n",
    "MANIFEST_FILE = f'{OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json'\n",
    "\n",
    "# Read the file and load all lines\n",
    "with open(MANIFEST_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Select 10 random lines from the file\n",
    "random_samples = random.sample(lines, min(10, len(lines)))\n",
    "\n",
    "# Process and display the 10 random samples\n",
    "for line in random_samples:\n",
    "    x = json.loads(line.strip())\n",
    "    display(Audio(x['audio_filepath']))\n",
    "    time.sleep(1)\n",
    "    print('Original: ')\n",
    "    print(x['text_no_preprocessing'])\n",
    "    print('Ground Truth: ')\n",
    "    print(x['text'])\n",
    "    print(f'Best hypothesis from {x[\"model_name\"]}')\n",
    "    print(x['pred_text'])\n",
    "    print(f\"WER : {x['WER']}, CER: {x['CER']}, Start CER: {x['start_CER']}, End CER: {x['end_CER']}, Alignment score: {x['score']}\")\n",
    "    print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the results were satisfactory, run the following script to upsample the clips to 44.1 kHz and create a metadata.csv file for the dataset, then zip the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"CTC_Dataset\"\n",
    "ZIP_PATH = f\"{DATASET_DIR}.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash $TOOLS_DIR/../run_prepare_dataset.sh \\\n",
    "--INPUT_AUDIO_DIR=$DATA_DIR/audio \\\n",
    "--MANIFEST=$OUTPUT_DIR/manifests/manifest_transcribed_metrics_filtered.json \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--OUTPUT_DIR=$OUTPUT_DIR \\\n",
    "--OUTPUT_FORMAT=$OUTPUT_FORMAT \\\n",
    "--MODE=$MODE \\\n",
    "--DATASET_DIR=$DATASET_DIR || exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
