{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Python cell to clone the repository\n",
    "!git clone https://github.com/saeedzou/NeMo.git\n",
    "%cd NeMo\n",
    "\n",
    "import os\n",
    "\n",
    "NEMO_DIR_PATH = \"./\"\n",
    "TOOLS_DIR = f'{NEMO_DIR_PATH}/tools/ctc_segmentation/scripts'\n",
    "WORK_DIR = 'WORK_DIR'\n",
    "DATA_DIR = WORK_DIR + '/DATA'\n",
    "OUTPUT_DIR = WORK_DIR + \"/output\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/audio', exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/text', exist_ok=True)\n",
    "\n",
    "! bash colab_install.sh\n",
    "! apt-get install -y sox libsox-fmt-mp3 ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data in the following format:\n",
    "- Audios must be under {DATA_DIR}/audio\n",
    "- Texts must be under {DATA_DIR}/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Define the models and paths\n",
    "NVIDIA_FASTCONFORMER = \"nvidia/stt_fa_fastconformer_hybrid_large\"\n",
    "WAV2VEC2_FA = \"masoudmzb/wav2vec2-xlsr-multilingual-53-fa\"\n",
    "HEZAR = 'hezarai/whisper-small-fa'\n",
    "VOSK_SMALL = 'vosk-model-small-fa-0.42'\n",
    "VOSK_BIG = 'vosk-model-fa-0.42'\n",
    "MODELS = f\"{NVIDIA_FASTCONFORMER} {WAV2VEC2_FA} {HEZAR}\"\n",
    "\n",
    "# List of (DATASET_DIR, GDRIVE_ID) pairs\n",
    "dataset_pairs = [\n",
    "    (\"dataset1\", \"GDRIVE_ID_1\"),\n",
    "    # (\"dataset2\", \"GDRIVE_ID_2\"),\n",
    "    # (\"dataset3\", \"GDRIVE_ID_3\"),\n",
    "]\n",
    "\n",
    "# Path to store output log file\n",
    "log_file = \"/content/processing_output.log\"\n",
    "# Define other variables\n",
    "LANG_ID = 'fa'\n",
    "THRESHOLD = -2\n",
    "CER_THRESHOLD = 40\n",
    "WER_THRESHOLD = 75\n",
    "CER_EDGE_THRESHOLD = 75\n",
    "LEN_DIFF_RATIO_THRESHOLD = 0.4\n",
    "MIN_DURATION = 1\n",
    "MAX_DURATION = 20\n",
    "EDGE_LEN = 7\n",
    "OUTPUT_FORMAT = 'wav'\n",
    "MODE = \"ganjoor\"\n",
    "REMOVE_BRACKETS = False\n",
    "REMOVE_ASTERISKS = False\n",
    "REMOVE_PARENTHESES = False\n",
    "REMOVE_SPEAKER_LABELS = False\n",
    "SPLIT_USING_PATTERN = True\n",
    "SPLIT_ON_QUOTES = False\n",
    "SPLIT_ON_VERBS = True\n",
    "SPLIT_ON_VERBS_MIN_WORDS = 5\n",
    "SPLIT_ON_VERBS_MAX_WORDS = 24\n",
    "ADDITIONAL_SPLIT_SYMBOLS = \"\" # add new symbols, separated by | (\\| before ? and ! and . and | because they are special characters in regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_api(id):\n",
    "    url = f'https://api.ganjoor.net/api/audio/verses/{id}'\n",
    "    response = requests.get(url)\n",
    "    # Check if the response status is OK (200)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        # If the request failed, return a failure message\n",
    "        return f\"Error: Unable to fetch poem. Status code: {response.status_code}\"\n",
    "\n",
    "\n",
    "# Function to run the bash scripts for each dataset\n",
    "def process_dataset(dataset_dir, gdrive_id):\n",
    "    zip_path = f\"/content/NeMo/{dataset_dir}.zip\"\n",
    "    WORK_DIR = 'WORK_DIR'\n",
    "    data_dir = WORK_DIR + '/DATA'\n",
    "    output_dir = WORK_DIR + \"/output\"\n",
    "    \n",
    "    # Writing the separator for the current dataset pair\n",
    "    print(f\"{'='*20}\\nProcessing dataset: {dataset_dir} with GDrive ID: {gdrive_id}\\n{'='*20}\")\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"\\n{'='*20}\\nProcessing dataset: {dataset_dir} with GDrive ID: {gdrive_id}\\n{'='*20}\\n\")\n",
    "\n",
    "    # Download and unzip the dataset\n",
    "    !gdown $gdrive_id -O $data_dir/audio.zip >> $log_file 2>&1\n",
    "    !unzip -q $data_dir/audio.zip -d $data_dir >> $log_file 2>&1\n",
    "\n",
    "\n",
    "    # Remove the output directory if it exists\n",
    "    !rm -rf $output_dir\n",
    "    \n",
    "    # remove texts because it doesn't contain punctuation\n",
    "    !rm -rf $DATA_DIR/text\n",
    "\n",
    "    os.makedirs(DATA_DIR + '/text', exist_ok=True)\n",
    "    ids = [int(f.split('.')[0]) for f in os.listdir(f'{DATA_DIR}/audio') if f.endswith('.mp3')]\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"Fetching text for {len(ids)} audio files\\n\")\n",
    "    for id in tqdm(ids):\n",
    "        text = get_text_api(id)\n",
    "        text = \"\\n\".join([x['verseText'] for x in text[1:]])\n",
    "        with open(f'{DATA_DIR}/text/{id}.txt', 'w') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    # Run the segmentation script\n",
    "    segmentation_command = f\"\"\"\n",
    "    !bash $TOOLS_DIR/../run_segmentation.sh \\\n",
    "    --MODEL_NAME_OR_PATH={NVIDIA_FASTCONFORMER} \\\n",
    "    --DATA_DIR={data_dir} \\\n",
    "    --OUTPUT_DIR={output_dir} \\\n",
    "    --SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "    --REMOVE_BRACKETS={REMOVE_BRACKETS} \\\n",
    "    --REMOVE_ASTERISKS={REMOVE_ASTERISKS} \\\n",
    "    --REMOVE_PARANTHESES={REMOVE_PARENTHESES} \\\n",
    "    --REMOVE_SPEAKER_LABELS={REMOVE_SPEAKER_LABELS} \\\n",
    "    --SPLIT_USING_PATTERN={SPLIT_USING_PATTERN} \\\n",
    "    --SPLIT_ON_QUOTES={SPLIT_ON_QUOTES} \\\n",
    "    --SPLIT_ON_VERBS={SPLIT_ON_VERBS} \\\n",
    "    --SPLIT_ON_VERBS_MIN_WORDS={SPLIT_ON_VERBS_MIN_WORDS} \\\n",
    "    --SPLIT_ON_VERBS_MAX_WORDS={SPLIT_ON_VERBS_MAX_WORDS} \\\n",
    "    --ADDITIONAL_SPLIT_SYMBOLS={ADDITIONAL_SPLIT_SYMBOLS} \\\n",
    "    --LANGUAGE={LANG_ID} \\\n",
    "    --MIN_SCORE={THRESHOLD} \\\n",
    "    --USE_NEMO_NORMALIZATION=False\n",
    "    \"\"\"\n",
    "    print(f\"{'-'*10}\\n Running CTC segmentation for {dataset_dir}\\n{'-'*10}\")\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(segmentation_command + \"\\n\")\n",
    "    !bash $TOOLS_DIR/../run_segmentation.sh \\\n",
    "    --MODEL_NAME_OR_PATH=$NVIDIA_FASTCONFORMER \\\n",
    "    --DATA_DIR=$data_dir \\\n",
    "    --OUTPUT_DIR=$output_dir \\\n",
    "    --SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "    --REMOVE_BRACKETS=$REMOVE_BRACKETS \\\n",
    "    --REMOVE_ASTERISKS=$REMOVE_ASTERISKS \\\n",
    "    --REMOVE_PARANTHESES=$REMOVE_PARENTHESES \\\n",
    "    --REMOVE_SPEAKER_LABELS=$REMOVE_SPEAKER_LABELS \\\n",
    "    --SPLIT_USING_PATTERN=$SPLIT_USING_PATTERN \\\n",
    "    --SPLIT_ON_QUOTES=$SPLIT_ON_QUOTES \\\n",
    "    --SPLIT_ON_VERBS=$SPLIT_ON_VERBS \\\n",
    "    --SPLIT_ON_VERBS_MIN_WORDS=$SPLIT_ON_VERBS_MIN_WORDS \\\n",
    "    --SPLIT_ON_VERBS_MAX_WORDS=$SPLIT_ON_VERBS_MAX_WORDS \\\n",
    "    --ADDITIONAL_SPLIT_SYMBOLS=$ADDITIONAL_SPLIT_SYMBOLS \\\n",
    "    --LANGUAGE=$LANG_ID \\\n",
    "    --MIN_SCORE=$THRESHOLD \\\n",
    "    --USE_NEMO_NORMALIZATION=False >> $log_file 2>&1\n",
    "\n",
    "    # Run the filtering script\n",
    "    filtering_command = f\"\"\"\n",
    "    !bash $TOOLS_DIR/../run_filter_multiple.sh \\\n",
    "    --MODEL_NAME_OR_PATH=\"{MODELS}\" \\\n",
    "    --INPUT_AUDIO_DIR={data_dir}/audio \\\n",
    "    --MANIFEST={output_dir}/manifests/manifest.json \\\n",
    "    --SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "    --CER_THRESHOLD={CER_THRESHOLD} \\\n",
    "    --WER_THRESHOLD={WER_THRESHOLD} \\\n",
    "    --CER_EDGE_THRESHOLD={CER_EDGE_THRESHOLD} \\\n",
    "    --LEN_DIFF_RATIO_THRESHOLD={LEN_DIFF_RATIO_THRESHOLD} \\\n",
    "    --MIN_DURATION={MIN_DURATION} \\\n",
    "    --MAX_DURATION={MAX_DURATION} \\\n",
    "    --EDGE_LEN={EDGE_LEN}\n",
    "    \"\"\"\n",
    "    print(f\"{'-'*10}\\n Running filtering by ASRs {MODELS} for {dataset_dir}\\n{'-'*10}\")\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(filtering_command + \"\\n\")\n",
    "    !bash $TOOLS_DIR/../run_filter_multiple.sh \\\n",
    "    --MODEL_NAME_OR_PATH=\"$MODELS\" \\\n",
    "    --INPUT_AUDIO_DIR=$data_dir/audio \\\n",
    "    --MANIFEST=$output_dir/manifests/manifest.json \\\n",
    "    --SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "    --CER_THRESHOLD=$CER_THRESHOLD \\\n",
    "    --WER_THRESHOLD=$WER_THRESHOLD \\\n",
    "    --CER_EDGE_THRESHOLD=$CER_EDGE_THRESHOLD \\\n",
    "    --LEN_DIFF_RATIO_THRESHOLD=$LEN_DIFF_RATIO_THRESHOLD \\\n",
    "    --MIN_DURATION=$MIN_DURATION \\\n",
    "    --MAX_DURATION=$MAX_DURATION \\\n",
    "    --EDGE_LEN=$EDGE_LEN >> $log_file 2>&1\n",
    "\n",
    "    # Process the JSON file\n",
    "    manifest_file = f'{output_dir}/manifests/manifest_transcribed_metrics_filtered.json'\n",
    "    with open(manifest_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    try:\n",
    "        num_lines = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if i % (num_lines // 10) == 0:\n",
    "                x = json.loads(line.strip())\n",
    "                display(Audio(x['audio_filepath']))\n",
    "                time.sleep(1)\n",
    "                print('Ground Truth: ')\n",
    "                print(x['text'])\n",
    "                print(f'Best hypothesis from {x[\"model_name\"]}')\n",
    "                print(x['pred_text'])\n",
    "                print(f\"WER : {x['WER']}, CER: {x['CER']}, Start CER: {x['start_CER']}, End CER: {x['end_CER']}, Alignment score: {x['score']}\")\n",
    "                print('*' * 20)\n",
    "    except:\n",
    "        print(\"Error in displaying the audio files\")\n",
    "\n",
    "    # Prepare the dataset\n",
    "    preparation_command = f\"\"\"\n",
    "    !bash $TOOLS_DIR/../run_prepare_dataset.sh \\\n",
    "    --INPUT_AUDIO_DIR={data_dir}/audio \\\n",
    "    --MANIFEST={output_dir}/manifests/manifest_transcribed_metrics_filtered.json \\\n",
    "    --SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "    --OUTPUT_DIR={output_dir} \\\n",
    "    --OUTPUT_FORMAT={OUTPUT_FORMAT} \\\n",
    "    --MODE={MODE} \\\n",
    "    --DATASET_DIR={dataset_dir}\n",
    "    \"\"\"\n",
    "    print(f\"{'-'*10}\\n Preparing dataset {dataset_dir}\\n{'-'*10}\")\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(preparation_command + \"\\n\")\n",
    "    !bash $TOOLS_DIR/../run_prepare_dataset.sh \\\n",
    "    --INPUT_AUDIO_DIR=$data_dir/audio \\\n",
    "    --MANIFEST=$output_dir/manifests/manifest_transcribed_metrics_filtered.json \\\n",
    "    --SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "    --OUTPUT_DIR=$output_dir \\\n",
    "    --OUTPUT_FORMAT=$OUTPUT_FORMAT \\\n",
    "    --MODE=$MODE \\\n",
    "    --DATASET_DIR=$dataset_dir >> $log_file 2>&1\n",
    "\n",
    "    # Save dataset to Google Drive\n",
    "    %cd /content\n",
    "    !mkdir -p /content/drive/MyDrive/Ganjoor_Dataset\n",
    "    !cp $zip_path /content/drive/MyDrive/Ganjoor_Dataset >> $log_file 2>&1\n",
    "    # copy logfile\n",
    "    !cp -f $log_file /content/drive/MyDrive/Ganjoor_Dataset >> $log_file 2>&1\n",
    "    print(f\"Dataset is saved in /content/drive/MyDrive/Ganjoor_Dataset/{dataset_dir}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataset pairs and process each\n",
    "for i, (dataset_dir, gdrive_id) in enumerate(dataset_pairs):\n",
    "    if i != 0:\n",
    "        !rm -rf /content/NeMo/WORK_DIR/DATA/audio/*\n",
    "        !rm -rf /content/NeMo/WORK_DIR/DATA/text/*\n",
    "        !rm -rf /content/NeMo/WORK_DIR/DATA/audio.zip\n",
    "        %cd NeMo\n",
    "    process_dataset(dataset_dir, gdrive_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
