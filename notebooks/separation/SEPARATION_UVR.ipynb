{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
    "# !pip install onnxruntime\n",
    "!pip -q install pydub\n",
    "!wget https://github.com/TRvlvr/model_repo/releases/download/all_public_uvr_models/UVR-MDX-NET-Inst_HQ_3.onnx\n",
    "!wget https://github.com/microsoft/DNS-Challenge/raw/refs/heads/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import librosa\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "INPUT_LENGTH = 9.01\n",
    "\n",
    "\n",
    "def source_separation(predictor, audio):\n",
    "    \"\"\"\n",
    "    Separate the audio into vocals and non-vocals using the given predictor.\n",
    "\n",
    "    Args:\n",
    "        predictor: The separation model predictor.\n",
    "        audio (str or dict): The audio file path or a dictionary containing audio waveform and sample rate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the separated vocals and updated audio waveform.\n",
    "    \"\"\"\n",
    "\n",
    "    mix, rate = None, None\n",
    "\n",
    "    if isinstance(audio, str):\n",
    "        mix, rate = librosa.load(audio, mono=False, sr=44100)\n",
    "    else:\n",
    "        # resample to 44100\n",
    "        rate = audio[\"sample_rate\"]\n",
    "        mix = librosa.resample(audio[\"waveform\"], orig_sr=rate, target_sr=44100)\n",
    "\n",
    "    vocals, no_vocals = predictor.predict(mix)\n",
    "\n",
    "    # convert vocals back to previous sample rate\n",
    "    # print(f\"vocals shape before resample: {vocals.shape}\")\n",
    "    vocals = librosa.resample(vocals.T, orig_sr=44100, target_sr=rate).T\n",
    "    no_vocals = librosa.resample(no_vocals.T, orig_sr=44100, target_sr=rate).T\n",
    "    # print(f\"vocals shape after resample: {vocals.shape}\")\n",
    "    audio[\"waveform\"] = vocals[:, 0]  # vocals is stereo, only use one channel\n",
    "    audio[\"other_waveform\"] = no_vocals[:, 0]  # no_vocals is stereo, only use one channel\n",
    "\n",
    "    return audio\n",
    "\n",
    "class ConvTDFNet:\n",
    "    \"\"\"\n",
    "    ConvTDFNet - Convolutional Temporal Frequency Domain Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_name, L, dim_f, dim_t, n_fft, hop=1024):\n",
    "        \"\"\"\n",
    "        Initialize ConvTDFNet.\n",
    "\n",
    "        Args:\n",
    "            target_name (str): The target name for separation.\n",
    "            L (int): Number of layers.\n",
    "            dim_f (int): Dimension in the frequency domain.\n",
    "            dim_t (int): Dimension in the time domain (log2).\n",
    "            n_fft (int): FFT size.\n",
    "            hop (int, optional): Hop size. Defaults to 1024.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(ConvTDFNet, self).__init__()\n",
    "        self.dim_c = 4\n",
    "        self.dim_f = dim_f\n",
    "        self.dim_t = 2**dim_t\n",
    "        self.n_fft = n_fft\n",
    "        self.hop = hop\n",
    "        self.n_bins = self.n_fft // 2 + 1\n",
    "        self.chunk_size = hop * (self.dim_t - 1)\n",
    "        self.window = torch.hann_window(window_length=self.n_fft, periodic=True)\n",
    "        self.target_name = target_name\n",
    "\n",
    "        out_c = self.dim_c * 4 if target_name == \"*\" else self.dim_c\n",
    "\n",
    "        self.freq_pad = torch.zeros([1, out_c, self.n_bins - self.dim_f, self.dim_t])\n",
    "        self.n = L // 2\n",
    "\n",
    "    def stft(self, x):\n",
    "        \"\"\"\n",
    "        Perform Short-Time Fourier Transform (STFT).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input waveform.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: STFT of the input waveform.\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1, self.chunk_size])\n",
    "        x = torch.stft(\n",
    "            x,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop,\n",
    "            window=self.window,\n",
    "            center=True,\n",
    "            return_complex=True,\n",
    "        )\n",
    "        x = torch.view_as_real(x)\n",
    "        x = x.permute([0, 3, 1, 2])\n",
    "        x = x.reshape([-1, 2, 2, self.n_bins, self.dim_t]).reshape(\n",
    "            [-1, self.dim_c, self.n_bins, self.dim_t]\n",
    "        )\n",
    "        return x[:, :, : self.dim_f]\n",
    "\n",
    "    def istft(self, x, freq_pad=None):\n",
    "        \"\"\"\n",
    "        Perform Inverse Short-Time Fourier Transform (ISTFT).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input STFT.\n",
    "            freq_pad (torch.Tensor, optional): Frequency padding. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Inverse STFT of the input.\n",
    "        \"\"\"\n",
    "        freq_pad = (\n",
    "            self.freq_pad.repeat([x.shape[0], 1, 1, 1])\n",
    "            if freq_pad is None\n",
    "            else freq_pad\n",
    "        )\n",
    "        x = torch.cat([x, freq_pad], -2)\n",
    "        c = 4 * 2 if self.target_name == \"*\" else 2\n",
    "        x = x.reshape([-1, c, 2, self.n_bins, self.dim_t]).reshape(\n",
    "            [-1, 2, self.n_bins, self.dim_t]\n",
    "        )\n",
    "        x = x.permute([0, 2, 3, 1])\n",
    "        x = x.contiguous()\n",
    "        x = torch.view_as_complex(x)\n",
    "        x = torch.istft(\n",
    "            x, n_fft=self.n_fft, hop_length=self.hop, window=self.window, center=True\n",
    "        )\n",
    "        return x.reshape([-1, c, self.chunk_size])\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Predictor class for source separation using ConvTDFNet and ONNX Runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, device):\n",
    "        \"\"\"\n",
    "        Initialize the Predictor.\n",
    "\n",
    "        Args:\n",
    "            args (dict): Configuration arguments.\n",
    "            device (str): Device to run the model ('cuda' or 'cpu').\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the provided device is not 'cuda' or 'cpu'.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.model_ = ConvTDFNet(\n",
    "            target_name=\"vocals\",\n",
    "            L=11,\n",
    "            dim_f=args[\"dim_f\"],\n",
    "            dim_t=args[\"dim_t\"],\n",
    "            n_fft=args[\"n_fft\"],\n",
    "        )\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            self.model = ort.InferenceSession(\n",
    "                args[\"model_path\"], providers=[\"CUDAExecutionProvider\"]\n",
    "            )\n",
    "        elif device == \"cpu\":\n",
    "            self.model = ort.InferenceSession(\n",
    "                args[\"model_path\"], providers=[\"CPUExecutionProvider\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Device must be either 'cuda' or 'cpu'\")\n",
    "\n",
    "    def demix(self, mix):\n",
    "        \"\"\"\n",
    "        Separate the sources from the input mix.\n",
    "\n",
    "        Args:\n",
    "            mix (np.ndarray): Input mixture signal.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Separated sources.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If margin is zero.\n",
    "        \"\"\"\n",
    "        samples = mix.shape[-1]\n",
    "        margin = self.args[\"margin\"]\n",
    "        chunk_size = self.args[\"chunks\"] * 44100\n",
    "\n",
    "        assert margin != 0, \"Margin cannot be zero!\"\n",
    "\n",
    "        if margin > chunk_size:\n",
    "            margin = chunk_size\n",
    "\n",
    "        segmented_mix = {}\n",
    "\n",
    "        if self.args[\"chunks\"] == 0 or samples < chunk_size:\n",
    "            chunk_size = samples\n",
    "\n",
    "        counter = -1\n",
    "        for skip in range(0, samples, chunk_size):\n",
    "            counter += 1\n",
    "            s_margin = 0 if counter == 0 else margin\n",
    "            end = min(skip + chunk_size + margin, samples)\n",
    "            start = skip - s_margin\n",
    "            segmented_mix[skip] = mix[:, start:end].copy()\n",
    "            if end == samples:\n",
    "                break\n",
    "\n",
    "        sources = self.demix_base(segmented_mix, margin_size=margin)\n",
    "        return sources\n",
    "\n",
    "    def demix_base(self, mixes, margin_size):\n",
    "        \"\"\"\n",
    "        Base function for source separation.\n",
    "\n",
    "        Args:\n",
    "            mixes (dict): Dictionary of segmented mixtures.\n",
    "            margin_size (int): Size of the margin.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Separated sources.\n",
    "        \"\"\"\n",
    "        chunked_sources = []\n",
    "        progress_bar = tqdm(total=len(mixes))\n",
    "        progress_bar.set_description(\"Source separation\")\n",
    "\n",
    "        for mix in mixes:\n",
    "            cmix = mixes[mix]\n",
    "            sources = []\n",
    "            n_sample = cmix.shape[1]\n",
    "            model = self.model_\n",
    "            trim = model.n_fft // 2\n",
    "            gen_size = model.chunk_size - 2 * trim\n",
    "            pad = gen_size - n_sample % gen_size\n",
    "            mix_p = np.concatenate(\n",
    "                (np.zeros((2, trim)), cmix, np.zeros((2, pad)), np.zeros((2, trim))), 1\n",
    "            )\n",
    "            mix_waves = []\n",
    "            i = 0\n",
    "            while i < n_sample + pad:\n",
    "                waves = np.array(mix_p[:, i : i + model.chunk_size])\n",
    "                mix_waves.append(waves)\n",
    "                i += gen_size\n",
    "\n",
    "            mix_waves = torch.tensor(np.array(mix_waves), dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _ort = self.model\n",
    "                spek = model.stft(mix_waves)\n",
    "                if self.args[\"denoise\"]:\n",
    "                    spec_pred = (\n",
    "                        -_ort.run(None, {\"input\": -spek.cpu().numpy()})[0] * 0.5\n",
    "                        + _ort.run(None, {\"input\": spek.cpu().numpy()})[0] * 0.5\n",
    "                    )\n",
    "                    tar_waves = model.istft(torch.tensor(spec_pred))\n",
    "                else:\n",
    "                    tar_waves = model.istft(\n",
    "                        torch.tensor(_ort.run(None, {\"input\": spek.cpu().numpy()})[0])\n",
    "                    )\n",
    "                tar_signal = (\n",
    "                    tar_waves[:, :, trim:-trim]\n",
    "                    .transpose(0, 1)\n",
    "                    .reshape(2, -1)\n",
    "                    .numpy()[:, :-pad]\n",
    "                )\n",
    "\n",
    "                start = 0 if mix == 0 else margin_size\n",
    "                end = None if mix == list(mixes.keys())[::-1][0] else -margin_size\n",
    "\n",
    "                if margin_size == 0:\n",
    "                    end = None\n",
    "\n",
    "                sources.append(tar_signal[:, start:end])\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "            chunked_sources.append(sources)\n",
    "        _sources = np.concatenate(chunked_sources, axis=-1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        return _sources\n",
    "\n",
    "    def predict(self, mix):\n",
    "        \"\"\"\n",
    "        Predict the separated sources from the input mix.\n",
    "\n",
    "        Args:\n",
    "            mix (np.ndarray): Input mixture signal.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the mixture minus the separated sources and the separated sources.\n",
    "        \"\"\"\n",
    "        if mix.ndim == 1:\n",
    "            mix = np.asfortranarray([mix, mix])\n",
    "\n",
    "        tail = mix.shape[1] % (self.args[\"chunks\"] * 44100)\n",
    "        if mix.shape[1] % (self.args[\"chunks\"] * 44100) != 0:\n",
    "            mix = np.pad(\n",
    "                mix,\n",
    "                (\n",
    "                    (0, 0),\n",
    "                    (\n",
    "                        0,\n",
    "                        self.args[\"chunks\"] * 44100\n",
    "                        - mix.shape[1] % (self.args[\"chunks\"] * 44100),\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        mix = mix.T\n",
    "        sources = self.demix(mix.T)\n",
    "        opt = sources[0].T\n",
    "\n",
    "        if tail != 0:\n",
    "            return ((mix - opt)[: -(self.args[\"chunks\"] * 44100 - tail), :], opt)\n",
    "        else:\n",
    "            return ((mix - opt), opt)\n",
    "\n",
    "\n",
    "class ComputeScore:\n",
    "    \"\"\"\n",
    "    ComputeScore class for evaluating DNSMOS.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, primary_model_path, device=\"cpu\") -> None:\n",
    "        if device == \"cuda\":\n",
    "            self.onnx_sess = ort.InferenceSession(\n",
    "                primary_model_path, providers=[\"CUDAExecutionProvider\"]\n",
    "            )\n",
    "            print(f\"Using CUDA: {self.onnx_sess.get_providers()}\")\n",
    "        else:\n",
    "            self.onnx_sess = ort.InferenceSession(primary_model_path)\n",
    "\n",
    "    def audio_melspec(self, audio, n_mels=120, frame_size=320, hop_length=160, sr=16000, to_db=True):\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=sr, n_fft=frame_size + 1, hop_length=hop_length, n_mels=n_mels\n",
    "        )\n",
    "        if to_db:\n",
    "            mel_spec = (librosa.power_to_db(mel_spec, ref=np.max) + 40) / 40\n",
    "        return mel_spec.T\n",
    "\n",
    "    def get_polyfit_val(self, sig, bak, ovr, is_personalized_MOS):\n",
    "        if is_personalized_MOS:\n",
    "            p_ovr = np.poly1d([-0.00533021, 0.005101, 1.18058466, -0.11236046])\n",
    "            p_sig = np.poly1d([-0.01019296, 0.02751166, 1.19576786, -0.24348726])\n",
    "            p_bak = np.poly1d([-0.04976499, 0.44276479, -0.1644611, 0.96883132])\n",
    "        else:\n",
    "            p_ovr = np.poly1d([-0.06766283, 1.11546468, 0.04602535])\n",
    "            p_sig = np.poly1d([-0.08397278, 1.22083953, 0.0052439])\n",
    "            p_bak = np.poly1d([-0.13166888, 1.60915514, -0.39604546])\n",
    "\n",
    "        return p_sig(sig), p_bak(bak), p_ovr(ovr)\n",
    "\n",
    "    def __call__(self, audio, sampling_rate, is_personalized_MOS=False):\n",
    "        fs = SAMPLING_RATE\n",
    "        if isinstance(audio, str):\n",
    "            audio, _ = librosa.load(audio, sr=fs)\n",
    "        elif sampling_rate != fs:\n",
    "            audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=fs)\n",
    "\n",
    "        actual_audio_len = len(audio)\n",
    "        len_samples = int(INPUT_LENGTH * fs)\n",
    "\n",
    "        while len(audio) < len_samples:\n",
    "            audio = np.append(audio, audio)\n",
    "\n",
    "        num_hops = int(np.floor(len(audio) / fs) - INPUT_LENGTH) + 1\n",
    "        hop_len_samples = fs\n",
    "\n",
    "        predicted_mos_sig_seg_raw = []\n",
    "        predicted_mos_bak_seg_raw = []\n",
    "        predicted_mos_ovr_seg_raw = []\n",
    "        predicted_mos_sig_seg = []\n",
    "        predicted_mos_bak_seg = []\n",
    "        predicted_mos_ovr_seg = []\n",
    "\n",
    "        for idx in range(num_hops):\n",
    "            audio_seg = audio[int(idx * hop_len_samples): int((idx + INPUT_LENGTH) * hop_len_samples)]\n",
    "            if len(audio_seg) < len_samples:\n",
    "                continue\n",
    "\n",
    "            input_features = np.array(audio_seg).astype(\"float32\")[np.newaxis, :]\n",
    "            oi = {\"input_1\": input_features}\n",
    "            mos_sig_raw, mos_bak_raw, mos_ovr_raw = self.onnx_sess.run(None, oi)[0][0]\n",
    "\n",
    "            mos_sig, mos_bak, mos_ovr = self.get_polyfit_val(\n",
    "                mos_sig_raw, mos_bak_raw, mos_ovr_raw, is_personalized_MOS\n",
    "            )\n",
    "\n",
    "            predicted_mos_sig_seg_raw.append(mos_sig_raw)\n",
    "            predicted_mos_bak_seg_raw.append(mos_bak_raw)\n",
    "            predicted_mos_ovr_seg_raw.append(mos_ovr_raw)\n",
    "            predicted_mos_sig_seg.append(mos_sig)\n",
    "            predicted_mos_bak_seg.append(mos_bak)\n",
    "            predicted_mos_ovr_seg.append(mos_ovr)\n",
    "\n",
    "        return {\n",
    "            \"filename\": \"audio_clip\",\n",
    "            \"len_in_sec\": actual_audio_len / fs,\n",
    "            \"sr\": fs,\n",
    "            \"num_hops\": num_hops,\n",
    "            \"OVRL_raw\": np.mean(predicted_mos_ovr_seg_raw),\n",
    "            \"SIG_raw\": np.mean(predicted_mos_sig_seg_raw),\n",
    "            \"BAK_raw\": np.mean(predicted_mos_bak_seg_raw),\n",
    "            \"OVRL\": np.mean(predicted_mos_ovr_seg),\n",
    "            \"SIG\": np.mean(predicted_mos_sig_seg),\n",
    "            \"BAK\": np.mean(predicted_mos_bak_seg),\n",
    "        }\n",
    "def mos_prediction(waveform, sampling_rate, dnsmos_compute_score):\n",
    "    dnsmos = dnsmos_compute_score(waveform, sampling_rate, False)['OVRL']\n",
    "    return dnsmos\n",
    "\n",
    "def standardization(audio, target_sr=24000, verbose=False):\n",
    "    \"\"\"\n",
    "    Preprocess the audio file, including setting sample rate, bit depth, channels, and volume normalization.\n",
    "\n",
    "    Args:\n",
    "        audio (str or AudioSegment): Audio file path or AudioSegment object, the audio to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed audio waveform, audio file name, and sample rate, formatted as:\n",
    "              {\n",
    "                  \"waveform\": np.ndarray, the preprocessed audio waveform, dtype is np.float32, shape is (num_samples,)\n",
    "                  \"name\": str, the audio file name\n",
    "                  \"sample_rate\": int, the audio sample rate\n",
    "              }\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the audio parameter is neither a str nor an AudioSegment.\n",
    "    \"\"\"\n",
    "    global audio_count\n",
    "    name = \"audio\"\n",
    "\n",
    "    if isinstance(audio, str):\n",
    "        name = os.path.basename(audio)\n",
    "        audio = AudioSegment.from_file(audio)\n",
    "    elif isinstance(audio, AudioSegment):\n",
    "        name = f\"audio_{audio_count}\"\n",
    "        audio_count += 1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid audio type\")\n",
    "\n",
    "    print(\"Entering the preprocessing of audio\") if verbose else None\n",
    "\n",
    "    # Convert the audio file to WAV format\n",
    "    audio = audio.set_frame_rate(target_sr)\n",
    "    audio = audio.set_sample_width(2)  # Set bit depth to 16bit\n",
    "    audio = audio.set_channels(1)  # Set to mono\n",
    "\n",
    "    print(\"Audio file converted to WAV format\") if verbose else None\n",
    "\n",
    "    # Calculate the gain to be applied\n",
    "    target_dBFS = -20\n",
    "    gain = target_dBFS - audio.dBFS\n",
    "    print(f\"Calculating the gain needed for the audio: {gain} dB\") if verbose else None\n",
    "\n",
    "    # Normalize volume and limit gain range to between -3 and 3\n",
    "    normalized_audio = audio.apply_gain(min(max(gain, -3), 3))\n",
    "\n",
    "    waveform = np.array(normalized_audio.get_array_of_samples(), dtype=np.float32)\n",
    "    max_amplitude = np.max(np.abs(waveform))\n",
    "    waveform /= max_amplitude  # Normalize\n",
    "\n",
    "    print(f\"waveform shape: {waveform.shape}\") if verbose else None\n",
    "    print(\"waveform in np ndarray, dtype=\" + str(waveform.dtype)) if verbose else None\n",
    "\n",
    "    return {\n",
    "        \"waveform\": waveform,\n",
    "        \"name\": name,\n",
    "        \"sample_rate\": target_sr,\n",
    "    }\n",
    "\n",
    "\n",
    "def write_mp3(path, sr, x):\n",
    "    \"\"\"Convert numpy array to MP3.\"\"\"\n",
    "    try:\n",
    "        # Ensure x is in the correct format and normalize if necessary\n",
    "        if x.dtype != np.int16:\n",
    "            # Normalize the array to fit in int16 range if it's not already int16\n",
    "            x = np.int16(x / np.max(np.abs(x)) * 32767)\n",
    "\n",
    "        # Create audio segment from numpy array\n",
    "        audio = AudioSegment(\n",
    "            x.tobytes(), frame_rate=sr, sample_width=x.dtype.itemsize, channels=1\n",
    "        )\n",
    "        # Export as MP3 file\n",
    "        audio.export(path, format=\"mp3\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error: Failed to write MP3 file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = \"cuda\"\n",
    "device = torch.device(device_name)\n",
    "cfg = {\n",
    "      \"model_path\": \"UVR-MDX-NET-Inst_HQ_3.onnx\",\n",
    "      \"denoise\": True,\n",
    "      \"margin\": 44100,\n",
    "      \"chunks\": 15,\n",
    "      \"n_fft\": 6144,\n",
    "      \"dim_t\": 8,\n",
    "      \"dim_f\": 3072\n",
    "      }\n",
    "\n",
    "dnsmos_compute_score = ComputeScore('sig_bak_ovr.onnx', device_name)\n",
    "\n",
    "separate_predictor1 = Predictor(\n",
    "        args=cfg, device=device_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'path'\n",
    "audio = standardization(audio_path)\n",
    "print(\"Before\")\n",
    "display(Audio(audio[\"waveform\"], rate=audio[\"sample_rate\"]))\n",
    "time.sleep(1)\n",
    "audio = source_separation(separate_predictor1, audio)\n",
    "print(\"After\")\n",
    "print('Vocals')\n",
    "display(Audio(audio[\"waveform\"], rate=audio[\"sample_rate\"]))\n",
    "print('Other')\n",
    "display(Audio(audio[\"other_waveform\"], rate=audio[\"sample_rate\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
