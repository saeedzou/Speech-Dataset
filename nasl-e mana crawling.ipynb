{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get install -y ffmpeg\n",
    "! sudo apt-get install -y zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import zipfile\n",
    "from google.colab import drive\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define the base URL and the directory to save files\n",
    "base_url = \"https://naslemana.com/\"\n",
    "\n",
    "part = 0\n",
    "step = 1\n",
    "\n",
    "save_dir = \"nasl-e mana/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(f\"{save_dir}/audio\", exist_ok=True)\n",
    "os.makedirs(f\"{save_dir}/text\", exist_ok=True)\n",
    "\n",
    "# Function to log messages to a file\n",
    "def log_to_file(message):\n",
    "    with open(log_file_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(f\"{message}\\n\")\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, save_path):\n",
    "    if not url.startswith(base_url):\n",
    "        log_to_file(f\"Link not starting with naslemana.com: {url}\")\n",
    "        return\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as out_file:\n",
    "        out_file.write(response.content)\n",
    "\n",
    "# Function to extract text from a page\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.find('span', class_='post-title', itemprop='headline').text\n",
    "    subtitle = soup.find('h2', class_='post-subtitle')\n",
    "    if subtitle:\n",
    "        subtitle = subtitle.text\n",
    "    else:\n",
    "        subtitle = \"\"\n",
    "    body_text = \" \".join([p.text for p in soup.find_all('div', class_='entry-content clearfix single-post-content')])\n",
    "    return title, subtitle, body_text\n",
    "\n",
    "\n",
    "# Function to get the duration of an audio file using ffmpeg\n",
    "def get_audio_duration(file_path):\n",
    "    try:\n",
    "        # Run ffmpeg to get the duration\n",
    "        result = subprocess.run(\n",
    "            ['ffmpeg', '-i', file_path],\n",
    "            stderr=subprocess.PIPE, stdout=subprocess.PIPE, text=True\n",
    "        )\n",
    "        output = result.stderr  # ffmpeg duration info is in stderr\n",
    "\n",
    "        # Find the duration in the output (look for \"Duration: \")\n",
    "        duration_line = [line for line in output.split('\\n') if \"Duration:\" in line]\n",
    "        if duration_line:\n",
    "            duration_text = duration_line[0].split(\"Duration:\")[1].split(\",\")[0].strip()\n",
    "            # Convert HH:MM:SS.MS to seconds\n",
    "            hours, minutes, seconds = map(float, duration_text.split(':'))\n",
    "            total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "            return total_seconds\n",
    "    except Exception as e:\n",
    "        log_to_file(f\"Error getting duration for {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# File path for log file\n",
    "log_file_path = f\"{save_dir}/crawl_log.txt\"\n",
    "\n",
    "# Initialize the metadata as a list\n",
    "metadata = []\n",
    "\n",
    "# Create the CSV file and write the header\n",
    "metadata_file_path = f\"{save_dir}/metadata.csv\"\n",
    "if not os.path.exists(metadata_file_path):\n",
    "    with open(metadata_file_path, mode=\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"magazine_name\", \"magazine_url\", \"subject\", \"audio_url\", \"audio_duration\", \"text_url\", \"file_name\"])\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the pages\n",
    "for page_num in range(1 + part * step, 1 + (part + 1) * step):\n",
    "    log_to_file(f\"Crawling page {page_num}\")\n",
    "    print(f\"Crawling page {page_num}\")\n",
    "    page_url = f\"{base_url}page/{page_num}/\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find candidate pages\n",
    "    candidate_pages = soup.find_all('a', class_='read-more')\n",
    "    for candidate in tqdm(candidate_pages):\n",
    "        log_to_file(f\"Crawling {candidate['href']}\")\n",
    "        candidate_url = candidate['href']\n",
    "        candidate_response = requests.get(candidate_url)\n",
    "        candidate_soup = BeautifulSoup(candidate_response.text, 'html.parser')\n",
    "\n",
    "        # Find elements with \"(صوت)\" and a link to an .mp3 file\n",
    "        audio_elements = candidate_soup.find_all('a', href=lambda href: href and href.endswith('.mp3'), string=lambda string: string and '(صوت)' in string)\n",
    "\n",
    "        if audio_elements:    # The candidate page is for a magazine\n",
    "          log_to_file(f\"Crawling {candidate_url}\")\n",
    "\n",
    "        for audio_element in tqdm(audio_elements):\n",
    "            audio_url = audio_element['href']\n",
    "            audio_text = audio_element.text.replace('(صوت)', '').strip()\n",
    "\n",
    "            # Look for a corresponding element with \"(متن)\"\n",
    "            text_element = candidate_soup.find('a', string=lambda string: string and '(متن)' in string and audio_text in string)\n",
    "            if text_element:\n",
    "                text_url = text_element['href']\n",
    "\n",
    "                # Extract text from the text page\n",
    "                title, subtitle, body_text = extract_text(text_url)\n",
    "\n",
    "                # Prepare the subject by removing specific words\n",
    "                subject = title.replace('(متن)', '').replace('(صوت)', '')\n",
    "\n",
    "                log_to_file(f\"Downloading the pair: {subject}\")\n",
    "\n",
    "                # Download the audio file\n",
    "                file_name = f\"{save_dir}/audio/{len(metadata) + 164}.mp3\"\n",
    "                download_file(audio_url, file_name)\n",
    "\n",
    "                # Save the text to a file\n",
    "                text_file_name = f\"{save_dir}/text/{len(metadata) + 164}.txt\"\n",
    "                with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"{title}\\n{subtitle}\\n{body_text}\")\n",
    "\n",
    "                # Update the metadata\n",
    "                metadata_entry = {\n",
    "                    \"magazine_name\": candidate_soup.find('span', class_='post-title', itemprop='headline').text,\n",
    "                    \"magazine_url\": candidate_url,\n",
    "                    \"subject\": subject,\n",
    "                    \"audio_url\": audio_url,\n",
    "                    'audio_duration': get_audio_duration(file_name),\n",
    "                    \"text_url\": text_url,\n",
    "                    \"file_name\": f\"{len(metadata) + 164}\"\n",
    "                }\n",
    "                metadata.append(metadata_entry)\n",
    "\n",
    "                # Append the metadata to the CSV file\n",
    "                with open(metadata_file_path, mode=\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=metadata_entry.keys())\n",
    "                    writer.writerow(metadata_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_folder_in_parts_with_progress(folder_path, zip_name, part_size_mb, part):\n",
    "    # List of all files in the folder for progress tracking\n",
    "    all_files = []\n",
    "    for foldername, subfolders, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(foldername, filename)\n",
    "            all_files.append(filepath)\n",
    "\n",
    "    # Create the base zip file and track progress\n",
    "    zip_filename = f\"{zip_name}_{part}.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for filepath in tqdm(all_files, desc=\"Zipping files\", unit=\"file\"):\n",
    "            arcname = os.path.relpath(filepath, folder_path)  # Create a relative path for the zip\n",
    "            zipf.write(filepath, arcname)\n",
    "\n",
    "    # Split the zip file into parts using the system zip tool\n",
    "    # The '-s' flag is used to split the zip file into smaller chunks\n",
    "    os.system(f\"zip -s {part_size_mb}m {zip_filename} {zip_filename}\")\n",
    "\n",
    "    # Clean up the original zip file after splitting\n",
    "    os.remove(zip_filename)\n",
    "\n",
    "    print(f\"Zip file successfully split into parts. Each part is {part_size_mb}MB.\")\n",
    "\n",
    "# Usage example\n",
    "folder_path = 'nasl-e-mana'\n",
    "zip_name = 'nasl-e-mana'\n",
    "part_size_mb = 2048  # Set to 2GB per part (adjust as needed)\n",
    "zip_folder_in_parts_with_progress(folder_path, zip_name, part_size_mb, part)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
