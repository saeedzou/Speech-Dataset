{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Authenticate to Google Drive to upload the dataset\n",
    "! wget -O colab_utils.py https://raw.githubusercontent.com/saeedzou/Speech-Dataset/main/colab_utils.py\n",
    "from colab_utils import initialize_drive, upload_file_to_drive, get_or_create_folder\n",
    "DRIVE = initialize_drive()\n",
    "\n",
    "\n",
    "! git clone https://github.com/saeedzou/NeMo.git\n",
    "%cd NeMo\n",
    "\n",
    "# Download cookies for gdown\n",
    "! gdown 11X7uwxGepuz62m_9A0o8XSqJjxeL_7OR\n",
    "! cp cookies.txt ~/.cache/gdown/cookies.txt\n",
    "\n",
    "NEMO_DIR_PATH = \"./\"\n",
    "TOOLS_DIR = f'{NEMO_DIR_PATH}/tools/ctc_segmentation/scripts'\n",
    "WORK_DIR = 'WORK_DIR'\n",
    "DATA_DIR = WORK_DIR + '/DATA'\n",
    "OUTPUT_DIR = WORK_DIR + \"/output\"\n",
    "\n",
    "import os\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/audio', exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/text', exist_ok=True)\n",
    "\n",
    "! bash colab_install.sh\n",
    "! apt-get install -y sox libsox-fmt-mp3 ffmpeg mediainfo\n",
    "! pip install -q pymediainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data in the following format:\n",
    "- Audios must be under {DATA_DIR}/audio\n",
    "- Texts must be under {DATA_DIR}/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/NeMo/WORK_DIR/DATA/audio/*\n",
    "!rm -rf /content/NeMo/WORK_DIR/DATA/text/*\n",
    "!rm -rf /content/NeMo/WORK_DIR/output\n",
    "!rm -rf /content/NeMo/WORK_DIR/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gdown\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import subprocess\n",
    "import ast\n",
    "import shutil\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "from pymediainfo import MediaInfo\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "NVIDIA_FASTCONFORMER = \"nvidia/stt_fa_fastconformer_hybrid_large\"\n",
    "WAV2VEC2_FA = \"masoudmzb/wav2vec2-xlsr-multilingual-53-fa\"\n",
    "WAV2VEC2_V3 = \"m3hrdadfi/wav2vec2-large-xlsr-persian-v3\"\n",
    "WHISPER_TINY = \"openai/whisper-tiny\"\n",
    "WHISPER_BASE = \"openai/whisper-base\"\n",
    "WHISPER_SMALL = \"openai/whisper-small\"\n",
    "WHISPER_MEDIUM = \"openai/whisper-medium\"\n",
    "WHISPER_LARGE = \"openai/whisper-large\"\n",
    "HEZAR = 'hezarai/whisper-small-fa'\n",
    "VOSK_SMALL = 'vosk-model-small-fa-0.42'\n",
    "VOSK_BIG = 'vosk-model-fa-0.42'\n",
    "MODELS_DICT = {\n",
    "    \"nvidia_stt_fa_fastconformer_hybrid_large\": \"NVIDIA_FASTCONFORMER\",\n",
    "    \"masoudmzb_wav2vec2_xlsr_multilingual_53_fa\": \"WAV2VEC2_FA\",\n",
    "    \"m3hrdadfi_wav2vec2_large_xlsr_persian_v3\": \"WAV2VEC2_V3\",\n",
    "    \"openai_whisper_tiny\": \"WHISPER_TINY\",\n",
    "    \"openai_whisper_base\": \"WHISPER_BASE\",\n",
    "    \"openai_whisper_small\": \"WHISPER_SMALL\",\n",
    "    \"openai_whisper_medium\": \"WHISPER_MEDIUM\",\n",
    "    \"openai_whisper_large\": \"WHISPER_LARGE\",\n",
    "    \"hezarai_whisper_small_fa\": \"HEZAR\",\n",
    "    \"vosk_model_small_fa_0_42\": \"VOSK_SMALL\",\n",
    "    \"vosk_model_fa_0_42\": \"VOSK_BIG\"\n",
    "}\n",
    "MODELS = f\"{NVIDIA_FASTCONFORMER} {WAV2VEC2_V3} {HEZAR}\"\n",
    "\n",
    "DATASET_DIR = \"\"\n",
    "IDX = 0\n",
    "MAX_DOWNLOADS_GDRIVE = 30\n",
    "\n",
    "MODE = \"\"\n",
    "LANG_ID='fa'\n",
    "OFFSET = 0\n",
    "THRESHOLD = -2\n",
    "WINDOW = 8000\n",
    "CER_THRESHOLD = 40\n",
    "WER_THRESHOLD = 75\n",
    "CER_EDGE_THRESHOLD = 75\n",
    "LEN_DIFF_RATIO_THRESHOLD = 0.4\n",
    "MIN_DURATION = 1\n",
    "MAX_DURATION = 20\n",
    "EDGE_LEN = 7\n",
    "OUTPUT_FORMAT = 'wav'\n",
    "REMOVE_BRACKETS=True\n",
    "REMOVE_ASTERISKS=True\n",
    "REMOVE_PARENTHESES=True\n",
    "REMOVE_SPEAKER_LABELS=True\n",
    "SPLIT_USING_PATTERN=False\n",
    "SPLIT_ON_QUOTES=False\n",
    "SPLIT_ON_VERBS=False\n",
    "ADDITIONAL_SPLIT_SYMBOLS=\"\"# add new symbols, separated by | (\\| before ? and ! and . and | because they are special characters in regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_persian(text, threshold=0.5):\n",
    "    # Regex for Persian characters\n",
    "    persian_chars = re.findall(r'[\\u0600-\\u06FF\\uFB50-\\uFDFF]', text)\n",
    "    total_chars = len(text)\n",
    "    \n",
    "    if total_chars == 0:  # Avoid division by zero\n",
    "        return False\n",
    "\n",
    "    # Calculate proportion of Persian characters\n",
    "    persian_ratio = len(persian_chars) / total_chars\n",
    "    return persian_ratio > threshold\n",
    "\n",
    "# Download excel files from google drive where each sheet corresponds to a channel\n",
    "# DATASET_DIR selects channel sheet from the excel file\n",
    "sheets_path = gdown.download(url='https://drive.google.com/uc?id=1qVw60j8Xhb1l1g4W8q0d8WlS5CaenNVMZH2q4AuLPjY', output=f\"{WORK_DIR}/sheets.xlsx\", quiet=False)\n",
    "sheets = pd.read_excel(sheets_path, sheet_name=None)\n",
    "\n",
    "# Download the metadata file and save texts in text folder\n",
    "summary = sheets['Summary']\n",
    "channel_metadata = summary[summary['Channel'] == DATASET_DIR]\n",
    "channel_metadata = gdown.download(url=channel_metadata['Gdrive Link'].values[0], output=f\"{WORK_DIR}/df_{IDX}.csv\", quiet=False)\n",
    "channel_metadata = pd.read_csv(channel_metadata)\n",
    "\n",
    "for i in range(IDX * MAX_DOWNLOADS_GDRIVE, min((IDX + 1) * MAX_DOWNLOADS_GDRIVE, len(channel_metadata))):\n",
    "    x = channel_metadata.iloc[i]\n",
    "    voice_name = x['voice_name']\n",
    "    basename = voice_name.split('.')[0]\n",
    "    text_name = basename + '.txt'\n",
    "    transcript = ast.literal_eval(x['transcript'])\n",
    "    transcript = pd.json_normalize(transcript)\n",
    "    #text = \"\\n\".join(transcript['text'].str.strip().str.replace(\"\\n\", \" \", regex=True))\n",
    "    text = \"\\n\".join(transcript['text'])\n",
    "    if is_persian(text):\n",
    "        with open(os.path.join(f\"{DATA_DIR}/text\", text_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "channel = sheets[DATASET_DIR]\n",
    "# channel contains Audio Name, GDrive Public Link\n",
    "# download all the audio files in range\n",
    "for index, row in channel.iterrows():\n",
    "    audio_name = row['Audio Name']\n",
    "    audio_link = row['GDrive Public Link']\n",
    "    audio_path = f\"{DATA_DIR}/audio/{audio_name}\"\n",
    "    # check if the audio file is already downloaded and the text file in the text folder is present\n",
    "    if not os.path.exists(audio_path) and os.path.exists(f\"{DATA_DIR}/text/{audio_name.split('.')[0]}.txt\"):\n",
    "        gdown.download(url=audio_link, output=audio_path, quiet=False)\n",
    "\n",
    "# Process each .wav file in the directory\n",
    "for file_name in tqdm(os.listdir(f\"{DATA_DIR}/audio\")):\n",
    "    if file_name.endswith(\".wav\"):\n",
    "        file_path = os.path.join(f\"{DATA_DIR}/audio\", file_name)\n",
    "        \n",
    "        try:\n",
    "            # Get media information\n",
    "            media_info = MediaInfo.parse(file_path)\n",
    "            format_detected = media_info.tracks[0].format if media_info.tracks else \"Unknown\"\n",
    "            \n",
    "            # Check if the file is mislabeled\n",
    "            if format_detected.lower() not in [\"wav\", \"pcm\"]:\n",
    "                # Define a temporary file path for the corrected file\n",
    "                temp_file_path = file_path.replace(\".wav\", \"_temp.wav\")\n",
    "                \n",
    "                # Use ffmpeg to convert to a proper .wav file (this is the temp file)\n",
    "                subprocess.run(\n",
    "                    [\"ffmpeg\", \"-i\", file_path, \"-ac\", \"1\", \"-c:a\", \"pcm_s16le\", temp_file_path],\n",
    "                    check=True,\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE\n",
    "                )\n",
    "                \n",
    "                # Remove the old file (mislabeled)\n",
    "                os.remove(file_path)\n",
    "                \n",
    "                # Rename the temporary file to overwrite the original\n",
    "                os.rename(temp_file_path, file_path)\n",
    "                \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"FFmpeg error processing {file_name}: {e.stderr.decode()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script does the following:\n",
    "1. Prepares data in the right format for CTC segmentation i.e. newline separated text (roughly an utterance) and 16000 Hz mono audio in `.wav` format for the NeMo ASR model\n",
    "\n",
    "2. Runs CTC segmentation on the processed data and outputs segments text file for each audio file containing utterance start, end timings and alignment score\n",
    "\n",
    "3. Verifies the segments created in step 2\n",
    "\n",
    "4. Cuts the audios into utterances and creates a json manifest file (NeMo format) of the information of each utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $OUTPUT_DIR\n",
    "\n",
    "! bash $TOOLS_DIR/../run_segmentation.sh \\\n",
    "--MODEL_NAME_OR_PATH=$NVIDIA_FASTCONFORMER \\\n",
    "--DATA_DIR=$DATA_DIR \\\n",
    "--OUTPUT_DIR=$OUTPUT_DIR \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--REMOVE_BRACKETS=$REMOVE_BRACKETS \\\n",
    "--REMOVE_ASTERISKS=$REMOVE_ASTERISKS \\\n",
    "--REMOVE_PARANTHESES=$REMOVE_PARENTHESES \\\n",
    "--REMOVE_SPEAKER_LABELS=$REMOVE_SPEAKER_LABELS \\\n",
    "--SPLIT_USING_PATTERN=$SPLIT_USING_PATTERN \\\n",
    "--SPLIT_ON_QUOTES=$SPLIT_ON_QUOTES \\\n",
    "--SPLIT_ON_VERBS=$SPLIT_ON_VERBS \\\n",
    "--ADDITIONAL_SPLIT_SYMBOLS=$ADDITIONAL_SPLIT_SYMBOLS \\\n",
    "--LANGUAGE=$LANG_ID \\\n",
    "--MIN_SCORE=$THRESHOLD  \\\n",
    "--USE_NEMO_NORMALIZATION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Transcribes the segments created from the `run_segmentation.sh` using the generated manifest for models in `MODELS` and calculate metrics such as WER, CER, etc. and outputs a new manifest file for each model\n",
    "\n",
    "2. Filters out segments that don't meet the minimum requirements of any of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash $TOOLS_DIR/../run_filter_multiple.sh \\\n",
    "--MODEL_NAME_OR_PATH=\"$MODELS\" \\\n",
    "--INPUT_AUDIO_DIR=$DATA_DIR/audio \\\n",
    "--MANIFEST=$OUTPUT_DIR/manifests/manifest.json \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--CER_THRESHOLD=$CER_THRESHOLD \\\n",
    "--WER_THRESHOLD=$WER_THRESHOLD \\\n",
    "--CER_EDGE_THRESHOLD=$CER_EDGE_THRESHOLD \\\n",
    "--LEN_DIFF_RATIO_THRESHOLD=$LEN_DIFF_RATIO_THRESHOLD \\\n",
    "--MIN_DURATION=$MIN_DURATION \\\n",
    "--MAX_DURATION=$MAX_DURATION \\\n",
    "--EDGE_LEN=$EDGE_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "MANIFEST_FILE = f'{OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json'\n",
    "\n",
    "# Read the file and load all lines\n",
    "with open(MANIFEST_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Select 10 random lines from the file\n",
    "random_samples = random.sample(lines, min(10, len(lines)))\n",
    "\n",
    "# Process and display the 10 random samples\n",
    "for line in random_samples:\n",
    "    x = json.loads(line.strip())\n",
    "    display(Audio(x['audio_filepath']))\n",
    "    time.sleep(1)\n",
    "    print('Original: ')\n",
    "    print(x['text_no_preprocessing'])\n",
    "    print('Ground Truth: ')\n",
    "    print(x['text'])\n",
    "    print(f'Best hypothesis from {x[\"model_name\"]}')\n",
    "    print(x['pred_text'])\n",
    "    print(f\"WER : {x['WER']}, CER: {x['CER']}, Start CER: {x['start_CER']}, End CER: {x['end_CER']}, Alignment score: {x['score']}\")\n",
    "    print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the results were satisfactory, run the following script to upsample the clips to 44.1 kHz and create a metadata.csv file for the dataset, then zip the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = f\"{DATASET_DIR}_{IDX}\"\n",
    "ZIP_PATH = f\"/content/NeMo/{DATASET_DIR}.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash $TOOLS_DIR/../run_prepare_dataset.sh \\\n",
    "--INPUT_AUDIO_DIR=$DATA_DIR/audio \\\n",
    "--MANIFEST=$OUTPUT_DIR/manifests/manifest_transcribed_metrics_filtered.json \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--OUTPUT_DIR=$OUTPUT_DIR \\\n",
    "--OUTPUT_FORMAT=$OUTPUT_FORMAT \\\n",
    "--MODE=$MODE \\\n",
    "--DATASET_DIR=\"$DATASET_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "CSV_FILE = f\"{DATASET_DIR}/metadata.csv\"\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "df['model_name'] = df['model_name'].map(MODELS_DICT)\n",
    "\n",
    "# Set the theme for the plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "sns.histplot(df['score'], bins=10, kde=True, color=\"lightgreen\", ax=axs, alpha=0.7)\n",
    "axs.set_xlabel('score', fontsize=12)\n",
    "axs.set_ylabel('Frequency', fontsize=12)\n",
    "axs.set_title('Distribution of CTC alignment score', fontsize=14)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.histplot(df['duration'], bins=10, kde=True, color=\"orchid\", ax=axs[0], alpha=0.7)\n",
    "axs[0].set_xlabel('duration', fontsize=12)\n",
    "axs[0].set_ylabel('Frequency', fontsize=12)\n",
    "axs[0].set_title('Distribution of duration', fontsize=14)\n",
    "\n",
    "sns.countplot(x='model_name', data=df, color='salmon', ax=axs[1])\n",
    "axs[1].set_xlabel('model_name', fontsize=12)\n",
    "axs[1].set_ylabel('Frequency', fontsize=12)\n",
    "axs[1].set_title('Distribution of model_name', fontsize=8)\n",
    "axs[1].tick_params(axis='x', rotation=60)\n",
    "\n",
    "# Plot histogram of CER and WER\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 5))\n",
    "sns.histplot(df['CER'], bins=10, kde=True, color=\"skyblue\", ax=axs[0], alpha=0.7)\n",
    "axs[0].set_xlabel('CER', fontsize=12)\n",
    "axs[0].set_ylabel('Frequency', fontsize=12)\n",
    "axs[0].set_title('Distribution of CER', fontsize=14)\n",
    "\n",
    "sns.histplot(df['WER'], bins=10, kde=True, color=\"salmon\", ax=axs[1], alpha=0.7)\n",
    "axs[1].set_xlabel('WER', fontsize=12)\n",
    "axs[1].set_ylabel('Frequency', fontsize=12)\n",
    "axs[1].set_title('Distribution of WER', fontsize=14)\n",
    "\n",
    "sns.histplot(df['start_CER'], bins=10, kde=True, color=\"lightgreen\", ax=axs[2], alpha=0.7)\n",
    "axs[2].set_xlabel('Start CER', fontsize=12)\n",
    "axs[2].set_ylabel('Frequency', fontsize=12)\n",
    "axs[2].set_title('Distribution of Start CER', fontsize=14)\n",
    "\n",
    "sns.histplot(df['end_CER'], bins=10, kde=True, color=\"gold\", ax=axs[3], alpha=0.7)\n",
    "axs[3].set_xlabel('End CER', fontsize=12)\n",
    "axs[3].set_ylabel('Frequency', fontsize=12)\n",
    "axs[3].set_title('Distribution of End CER', fontsize=14)\n",
    "\n",
    "# Plot histogram of ins_rate, del_rate, sub_rate\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "sns.histplot(df['ins_rate'], bins=10, kde=True, color=\"lightgreen\", ax=axs[0], alpha=0.7)\n",
    "axs[0].set_xlabel('Insertion Rate', fontsize=12)\n",
    "axs[0].set_ylabel('Frequency', fontsize=12)\n",
    "axs[0].set_title('Distribution of Insertion Rate', fontsize=14)\n",
    "\n",
    "sns.histplot(df['del_rate'], bins=10, kde=True, color=\"gold\", ax=axs[1], alpha=0.7)\n",
    "axs[1].set_xlabel('Deletion Rate', fontsize=12)\n",
    "axs[1].set_ylabel('Frequency', fontsize=12)\n",
    "axs[1].set_title('Distribution of Deletion Rate', fontsize=14)\n",
    "\n",
    "sns.histplot(df['sub_rate'], bins=10, kde=True, color=\"orchid\", ax=axs[2], alpha=0.7)\n",
    "axs[2].set_xlabel('Substitution Rate', fontsize=12)\n",
    "axs[2].set_ylabel('Frequency', fontsize=12)\n",
    "axs[2].set_title('Distribution of Substitution Rate', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder_id = get_or_create_folder(DRIVE, 'root', 'Youtube_Dataset')\n",
    "zip_id = upload_file_to_drive(DRIVE, parent_folder_id, ZIP_PATH, os.path.basename(ZIP_PATH))\n",
    "print(f\"*** Dataset {DATASET_DIR} created ***\")\n",
    "print(f\"*** Drive ID: {zip_id} ***\")\n",
    "print(f\"*** Drive link: https://drive.google.com/file/d/{zip_id}/view?usp=sharing ***\")\n",
    "print(f\"*** Direct download link: https://drive.google.com/uc?id={zip_id} ***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
