{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Python cell to clone the repository\n",
    "!gdown 11X7uwxGepuz62m_9A0o8XSqJjxeL_7OR\n",
    "!cp cookies.txt ~/.cache/gdown/cookies.txt\n",
    "!git clone https://github.com/saeedzou/NeMo.git\n",
    "%cd NeMo\n",
    "\n",
    "import os\n",
    "\n",
    "NEMO_DIR_PATH = \"./\"\n",
    "TOOLS_DIR = f'{NEMO_DIR_PATH}/tools/ctc_segmentation/scripts'\n",
    "WORK_DIR = 'WORK_DIR'\n",
    "DATA_DIR = WORK_DIR + '/DATA'\n",
    "OUTPUT_DIR = WORK_DIR + \"/output\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/audio', exist_ok=True)\n",
    "os.makedirs(DATA_DIR + '/text', exist_ok=True)\n",
    "\n",
    "! bash colab_install.sh\n",
    "! apt-get install -y sox libsox-fmt-mp3 ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your data in the following format:\n",
    "- Audios must be under {DATA_DIR}/audio\n",
    "- Texts must be under {DATA_DIR}/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "import gdown\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import shutil\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "NVIDIA_FASTCONFORMER = \"nvidia/stt_fa_fastconformer_hybrid_large\"\n",
    "WAV2VEC2_FA = \"masoudmzb/wav2vec2-xlsr-multilingual-53-fa\"\n",
    "WAV2VEC2_V3 = \"m3hrdadfi/wav2vec2-large-xlsr-persian-v3\"\n",
    "WHISPER_TINY = \"openai/whisper-tiny\"\n",
    "WHISPER_BASE = \"openai/whisper-base\"\n",
    "WHISPER_SMALL = \"openai/whisper-small\"\n",
    "WHISPER_MEDIUM = \"openai/whisper-medium\"\n",
    "WHISPER_LARGE = \"openai/whisper-large\"\n",
    "HEZAR = 'hezarai/whisper-small-fa'\n",
    "VOSK_SMALL = 'vosk-model-small-fa-0.42'\n",
    "VOSK_BIG = 'vosk-model-fa-0.42'\n",
    "MODELS = f\"{NVIDIA_FASTCONFORMER} {WAV2VEC2_FA} {HEZAR}\"\n",
    "\n",
    "DATASET_DIR = \"\"\n",
    "\n",
    "MODE = \"\"\n",
    "LANG_ID='fa'\n",
    "OFFSET = 0\n",
    "THRESHOLD = -2\n",
    "WINDOW = 8000\n",
    "CER_THRESHOLD = 40\n",
    "WER_THRESHOLD = 75\n",
    "CER_EDGE_THRESHOLD = 75\n",
    "LEN_DIFF_RATIO_THRESHOLD = 0.4\n",
    "MIN_DURATION = 1\n",
    "MAX_DURATION = 20\n",
    "EDGE_LEN = 7\n",
    "OUTPUT_FORMAT = 'mp3'\n",
    "REMOVE_BRACKETS=True\n",
    "REMOVE_ASTERISKS=True\n",
    "REMOVE_PARENTHESES=True\n",
    "REMOVE_SPEAKER_LABELS=True\n",
    "SPLIT_USING_PATTERN=False\n",
    "SPLIT_ON_QUOTES=False\n",
    "SPLIT_ON_VERBS=False\n",
    "ADDITIONAL_SPLIT_SYMBOLS=\":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/content'\n",
    "channels_csv_path = 'youtube_links.csv'\n",
    "\n",
    "channels_df = pd.read_csv(channels_csv_path)\n",
    "idx = 10 # index of the channel in the csv\n",
    "gdown.download_folder(url=channels_df.loc[idx]['link'], quiet=False, remaining_ok=True)\n",
    "\n",
    "channel_dir = os.path.join(root_dir, channels_df.loc[idx]['name'])\n",
    "channel_audio_dir = os.path.join(channel_dir, 'Audios')\n",
    "audio_dir = \"/content/NeMo/WORK_DIR/DATA/audio/\"\n",
    "text_dir = \"/content/NeMo/WORK_DIR/DATA/text\"\n",
    "df_file = os.path.join(channel_dir, \"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all files in channel_dir (without copying the folder itself)\n",
    "for item in os.listdir(channel_audio_dir):\n",
    "    source_path = os.path.join(channel_audio_dir, item)\n",
    "    if os.path.isfile(source_path):  # Only copy files\n",
    "        shutil.copy(source_path, audio_dir)\n",
    "\n",
    "# Process each .wav file in the directory\n",
    "for file_name in tqdm(os.listdir(audio_dir)):\n",
    "    # Check if the file is a .wav file\n",
    "    if file_name.endswith(\".wav\"):\n",
    "        wav_path = os.path.join(audio_dir, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Load the audio file with librosa\n",
    "            audio, sr = librosa.load(wav_path, sr=None)  # sr=None preserves the original sampling rate\n",
    "            \n",
    "            # Write the audio data to an MP3 file using soundfile\n",
    "            sf.write(wav_path, audio, sr, format=\"WAV\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_file)\n",
    "\n",
    "for i in range(len(df)):\n",
    "  x = df.iloc[i]\n",
    "  voice_name = x['voice_name']\n",
    "  basename = voice_name.split('.')[0]\n",
    "  text_name = basename + '.txt'\n",
    "  transcript = ast.literal_eval(x['transcript'])\n",
    "  transcript = pd.json_normalize(transcript)\n",
    "  text = \"\\n\".join(transcript['text'])\n",
    "  with open(os.path.join(text_dir, text_name), 'w', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script does the following:\n",
    "1. Prepares data in the right format for CTC segmentation i.e. newline separated text (roughly an utterance) and 16000 Hz mono audio in `.wav` format for the NeMo ASR model\n",
    "\n",
    "2. Runs CTC segmentation on the processed data and outputs segments text file for each audio file containing utterance start, end timings and alignment score\n",
    "\n",
    "3. Verifies the segments created in step 2\n",
    "\n",
    "4. Cuts the audios into utterances and creates a json manifest file (NeMo format) of the information of each utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $OUTPUT_DIR\n",
    "\n",
    "! bash $TOOLS_DIR/../run_segmentation.sh \\\n",
    "--MODEL_NAME_OR_PATH=$NVIDIA_FASTCONFORMER \\\n",
    "--DATA_DIR=$DATA_DIR \\\n",
    "--OUTPUT_DIR=$OUTPUT_DIR \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--REMOVE_BRACKETS=$REMOVE_BRACKETS \\\n",
    "--REMOVE_ASTERISKS=$REMOVE_ASTERISKS \\\n",
    "--REMOVE_PARANTHESES=$REMOVE_PARANTHESES \\\n",
    "--REMOVE_SPEAKER_LABELS=$REMOVE_SPEAKER_LABELS \\\n",
    "--SPLIT_USING_PATTERN=$SPLIT_USING_PATTERN \\\n",
    "--SPLIT_ON_QUOTES=$SPLIT_ON_QUOTES \\\n",
    "--SPLIT_ON_VERBS=$SPLIT_ON_VERBS \\\n",
    "--ADDITIONAL_SPLIT_SYMBOLS=$ADDITIONAL_SPLIT_SYMBOLS \\\n",
    "--LANGUAGE=$LANG_ID \\\n",
    "--MIN_SCORE=$THRESHOLD  \\\n",
    "--USE_NEMO_NORMALIZATION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Transcribes the segments created from the `run_segmentation.sh` using the generated manifest for models in `MODELS` and calculate metrics such as WER, CER, etc. and outputs a new manifest file for each model\n",
    "\n",
    "2. Filters out segments that don't meet the minimum requirements of any of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash $TOOLS_DIR/../run_filter_multiple.sh \\\n",
    "--MODEL_NAME_OR_PATH=\"$MODELS\" \\\n",
    "--INPUT_AUDIO_DIR=$DATA_DIR/audio \\\n",
    "--MANIFEST=$OUTPUT_DIR/manifests/manifest.json \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--CER_THRESHOLD=$CER_THRESHOLD \\\n",
    "--WER_THRESHOLD=$WER_THRESHOLD \\\n",
    "--CER_EDGE_THRESHOLD=$CER_EDGE_THRESHOLD \\\n",
    "--LEN_DIFF_RATIO_THRESHOLD=$LEN_DIFF_RATIO_THRESHOLD \\\n",
    "--MIN_DURATION=$MIN_DURATION \\\n",
    "--MAX_DURATION=$MAX_DURATION \\\n",
    "--EDGE_LEN=$EDGE_LEN || exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the JSON file\n",
    "MANIFEST_FILE = f'{OUTPUT_DIR}/manifests/manifest_transcribed_metrics_filtered.json'\n",
    "\n",
    "# Count the number of lines in the file\n",
    "with open(MANIFEST_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "num_lines = len(lines)  # Get the number of lines\n",
    "\n",
    "# Process the file line by line\n",
    "for i, line in enumerate(lines):\n",
    "    if i % (num_lines // 10) == 0:  # Print every 10% of the data\n",
    "        x = json.loads(line.strip())\n",
    "        display(Audio(x['audio_filepath']))\n",
    "        print('Ground Truth: ')\n",
    "        print(x['text'])\n",
    "        print(f'Best hypothesis from {x[\"model_name\"]}')\n",
    "        print(x['pred_text'])\n",
    "        print(f\"WER : {x['WER']}, CER: {x['CER']}, Start CER: {x['start_CER']}, End CER: {x['end_CER']}, Alignment score: {x['score']}\")\n",
    "        print('*' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the results were satisfactory, run the following script to upsample the clips to 44.1 kHz and create a metadata.csv file for the dataset, then zip the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash $TOOLS_DIR/../run_prepare_dataset.sh \\\n",
    "--INPUT_AUDIO_DIR=$DATA_DIR/audio \\\n",
    "--MANIFEST=$OUTPUT_DIR/manifests/manifest_transcribed_metrics_filtered.json \\\n",
    "--SCRIPTS_DIR=$TOOLS_DIR \\\n",
    "--OUTPUT_DIR=$OUTPUT_DIR \\\n",
    "--OUTPUT_FORMAT=$OUTPUT_FORMAT \\\n",
    "--MODE=$MODE \\\n",
    "--DATASET_DIR=$DATASET_DIR || exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
